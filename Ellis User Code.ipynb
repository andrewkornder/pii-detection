{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1bb5a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/ellisobrien/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing processing packages\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import missingno as miss\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "from nltk.corpus import words\n",
    "import nltk\n",
    "\n",
    "# Download the NLTK words corpus (only need to do this once)\n",
    "nltk.download('words')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#importing viz packages\n",
    "import plotly.express as px\n",
    "import plotly \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing machine learning packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4baa1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path where the JSON files are located\n",
    "directory = '/Users/ellisobrien/Desktop/Kaggle/Educational Data/pii-detection-removal-from-educational-data/'\n",
    "\n",
    "# Define the filenames for training and testing data\n",
    "train_file = 'train.json'\n",
    "test_file = 'test.json'\n",
    "\n",
    "# Construct the full file paths\n",
    "train_filepath = directory + train_file\n",
    "test_filepath = directory + test_file\n",
    "\n",
    "# Read the training data from JSON\n",
    "df_train = pd.read_json(train_filepath)\n",
    "\n",
    "# Read the testing data from JSON\n",
    "df_test = pd.read_json(test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "409bbf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      document                                          full_text  \\\n",
      "0            7  Design Thinking for innovation reflexion-Avril...   \n",
      "1           10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
      "2           16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n",
      "3           20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n",
      "4           56  Assignment:  Visualization Reflection  Submitt...   \n",
      "...        ...                                                ...   \n",
      "6802     22678  EXAMPLE – JOURNEY MAP\\n\\nTHE CHALLENGE    My w...   \n",
      "6803     22679  Why Mind Mapping?\\n\\nMind maps are graphical r...   \n",
      "6804     22681  Challenge\\n\\nSo, a few months back, I had chos...   \n",
      "6805     22684  Brainstorming\\n\\nChallenge & Selection\\n\\nBrai...   \n",
      "6806     22687  Mind Mapping\\n\\nChallenge\\n\\nMy consulting tea...   \n",
      "\n",
      "                                        filtered_tokens  \n",
      "0     [design, thinking, for, innovation, challenge,...  \n",
      "1     [design, thinking, assignment, visualization, ...  \n",
      "2     [process, by, challenge, i, received, a, promo...  \n",
      "3     [design, thinking, for, innovation, university...  \n",
      "4     [assignment, visualization, reflection, by, bo...  \n",
      "...                                                 ...  \n",
      "6802  [example, journey, map, the, challenge, my, wi...  \n",
      "6803  [why, mind, mind, are, graphical, of, informat...  \n",
      "6804  [challenge, so, a, few, back, i, had, chosen, ...  \n",
      "6805  [challenge, selection, is, a, very, powerful, ...  \n",
      "6806  [mind, challenge, my, consulting, and, i, spec...  \n",
      "\n",
      "[6807 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the English words corpus from NLTK\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Function to preprocess text data and filter out non-words\n",
    "def preprocess_and_filter(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, punctuation, and extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize the text into words\n",
    "    tokens = text.split()\n",
    "    # Filter out non-words\n",
    "    filtered_tokens = [token for token in tokens if token in english_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply the preprocess_and_filter function to the 'full_text' column\n",
    "df_train['filtered_tokens'] = df_train['full_text'].apply(preprocess_and_filter)\n",
    "\n",
    "# Display the DataFrame with filtered tokens\n",
    "print(df_train[['document', 'full_text', 'filtered_tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c342961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              full_text  \\\n",
      "0     Design Thinking for innovation reflexion-Avril...   \n",
      "1     Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
      "2     Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n",
      "3     Design Thinking for Innovation\\n\\nSindy Samaca...   \n",
      "4     Assignment:  Visualization Reflection  Submitt...   \n",
      "...                                                 ...   \n",
      "6802  EXAMPLE – JOURNEY MAP\\n\\nTHE CHALLENGE    My w...   \n",
      "6803  Why Mind Mapping?\\n\\nMind maps are graphical r...   \n",
      "6804  Challenge\\n\\nSo, a few months back, I had chos...   \n",
      "6805  Brainstorming\\n\\nChallenge & Selection\\n\\nBrai...   \n",
      "6806  Mind Mapping\\n\\nChallenge\\n\\nMy consulting tea...   \n",
      "\n",
      "                                        filtered_tokens  \n",
      "0     [design, thinking, for, innovation, challenge,...  \n",
      "1     [design, thinking, assignment, visualization, ...  \n",
      "2     [process, by, challenge, i, received, a, promo...  \n",
      "3     [design, thinking, for, innovation, university...  \n",
      "4     [assignment, visualization, reflection, by, bo...  \n",
      "...                                                 ...  \n",
      "6802  [example, journey, map, the, challenge, my, wi...  \n",
      "6803  [why, mind, mind, are, graphical, of, informat...  \n",
      "6804  [challenge, so, a, few, back, i, had, chosen, ...  \n",
      "6805  [challenge, selection, is, a, very, powerful, ...  \n",
      "6806  [mind, challenge, my, consulting, and, i, spec...  \n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to show only rows where potential_usernames is not blank\n",
    "non_empty_usernames_df = df_train[df_train['filtered_tokens'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(non_empty_usernames_df[['full_text', 'filtered_tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to identify potential usernames in a given text\n",
    "# def find_usernames(text):\n",
    "#     # Define regular expression pattern for identifying potential usernames\n",
    "#     username_pattern = r'\\b@\\w{3,20}\\b'\n",
    "    \n",
    "#     # Find all occurrences of potential usernames in the text\n",
    "#     potential_usernames = re.findall(username_pattern, text)\n",
    "    \n",
    "#     return potential_usernames\n",
    "\n",
    "# # Apply the find_usernames function to each row in the DataFrame\n",
    "# df_train['potential_usernames'] = df_train['full_text'].apply(find_usernames)\n",
    "\n",
    "# # Display the DataFrame with potential usernames identified\n",
    "# print(df_train[['full_text', 'potential_usernames']])\n",
    "\n",
    "\n",
    "# # Function to preprocess text data\n",
    "# def preprocess_text(text):\n",
    "#     # Convert text to lowercase\n",
    "#     text = text.lower()\n",
    "#     # Remove special characters, punctuation, and extra whitespace\n",
    "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "#     # Tokenize the text into words\n",
    "#     tokens = text.split()\n",
    "#     return tokens\n",
    "\n",
    "# # Apply the preprocess_text function to the 'full_text' column\n",
    "# df_train['tokens'] = df_train['full_text'].apply(preprocess_text)\n",
    "\n",
    "# # Display the DataFrame with tokens\n",
    "# print(df_train[['document', 'full_text', 'tokens']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
